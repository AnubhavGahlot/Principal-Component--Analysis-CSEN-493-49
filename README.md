CSEN 439-49 Directed Research
(Under the guidance of Prof. Behnam Dezfouli)


Leverage 
Principal Component Analysis to Efficiently Reduce IOT Data Sizes without Compromising their Accuracy

Knowledge & Roadmap Document
  
Introduction	

The Rise of the Internet of Things (IoT):
The Internet of Things (IoT) encapsulates a rapidly expanding network of devices equipped with sensors, The term "Internet of Things" (IoT) refers to a quickly growing network of gadgets with sensors, software, and connectivity that gather and share data in a variety of settings. 
IoT is essential to improving real-time analytics, machine learning, and autonomous decision-making in a variety of industries and healthcare applications, from consumer goods like smartwatches and home automation systems to complex industrial and monitoring instruments. 
In addition to improving operational efficiency, the widespread use of connected devices is changing many industries by providing faster and more precise insights. 

The IoT Data Deluge: Challenges and Bottlenecks:
The rapid rise of the Internet of Things generates large amounts of data, which are often high in velocity and variety, causing substantial logistical issues. The key concerns include the storage, management, and processing of this data. Data storage costs are rising as businesses strive to handle increasing volumes of information securely and reliably. Similarly, data transmission via networks has obstacles such as bandwidth constraints, which can impair data flow efficiency, especially in remote or congested locations. These bottlenecks are crucial because they cause delays in the processing and interpretation of real-time data, reducing the potential benefits of IoT deployments.

Data Reduction Methods
IoT data reduction can be accomplished using a variety of ways, each suited to the individual demands and features of the data involved. Data compression techniques are particularly important in this regard. Lossless compression algorithms such as Huffman coding and Lempel-Ziv-Welch (LZW) are essential when precision is required, as they ensure that no information is lost during the compression process. This is critical in situations where data integrity is unquestionable, such as in legal or financial documents. Lossy compression, on the other hand, prioritizes efficiency over exactness by deleting less important data, resulting in larger compression ratios. This method is appropriate for multimedia applications such as JPEG image compression and MP3 audio files, where minor quality compromises are tolerated in exchange for much smaller data quantities, benefiting services such as streaming. Data sampling methods also help to reduce data volume through strategies such as uniform sampling, which selects data points at predetermined intervals. Adaptive sampling improves on this strategy by dynamically altering sampling rates in response to data fluctuation, resulting in an optimal balance of data resolution and volume.
Another component of data reduction in IoT is data aggregation, which intentionally mixes data to reduce redundancy before transmission or storage. In-network aggregation combines data from many sources at the network level, generally by computing summary statistics such as averages or maximum values, greatly lowering the quantity of data that must be transferred. Temporal aggregation expands this notion by summarizing data across certain time periods, which is especially beneficial in applications that monitor environmental conditions or system performance and require periodic summaries rather than continuous streams. Aside from aggregation, feature selection and extraction are complex data reduction techniques that reduce the dataset to its most essential components. Feature selection makes further analysis easier by eliminating redundant or irrelevant information without changing the essential structure of the data. By applying mathematical transformations like principal component analysis (PCA) or Fourier transforms, feature extraction goes one step further and generates additional, more powerful features that improve the dataset's usefulness for predictive modeling and analysis. 
When compared to these methodologies, PCA has considerable advantages, especially for IoT applications. PCA extracts the principal components that account for the bulk of data variance, preserving the most important patterns and trends. This not only helps to maintain the data's quality and integrity, but it also improves the results' interpretability by offering clear insights into what factors have the greatest influence on data variability. Unlike approaches that may overlook key data details, such as basic compression or rudimentary sampling, PCA takes a controlled, methodical approach to data reduction. It can effectively reduce dataset dimensionality while preserving the data's structural integrity. This makes PCA an effective tool when combined with other reduction techniques, such as feature selection, because it can preprocess data to lower dimensions before more specialized feature extraction is applied. PCA outperforms autoencoders, another advanced reduction technique, in terms of computing efficiency and ease of interpretation, making it preferable in scenarios requiring clear, understandable findings as well as large data reduction.
When using PCA to reduce IoT data, several important factors must be considered. The technique is more effective when the variables in the dataset are highly correlated; however, its usefulness may be reduced if the data characteristics are generally independent. The choice of reduction method is also determined by the use case's specific requirements. The decision-making process is influenced by several factors, including the intended balance between data integrity and size reduction, computational resources, the underlying nature of the data, and how the reduced data will be used. As a result, while PCA is highly versatile and effective, it must be chosen with a full grasp of the underlying data characteristics and the strategic goals of the IoT application under consideration.
Principal Component Analysis (PCA): A Potential Solution for Data Reduction:
Principal Component Analysis (PCA) reduces the dimensionality of IoT data, providing a strategic way to lessen the impact of these issues. Less informative characteristics can be minimized or eliminated thanks to PCA's ability to identify the most important data features that contribute to variation. The data are changed through this procedure into a set of main components, which are a collection of linearly uncorrelated variables. The majority of the data is usually retained by the first few principal components, enabling a condensed yet useful representation of the original dataset. Effective management and analysis of IoT data is made simpler by this decrease, which also speeds up processing times and simplifies data transfer and storage. 
A crucial field of study that deals with the processing, storing, and analyzing of data produced by a wide range of connected devices is the management of Internet of Things data. Research in this area has looked into a number of topics, including as energy-efficient data transmission strategies, real-time data processing frameworks, and data compression approaches. For instance, studies have shown how to lower the latency and bandwidth needs of conventional cloud storage solutions by utilizing edge computing and sophisticated compression methods. In an attempt to reduce storage requirements without sacrificing important insights, several studies have suggested machine learning-based methods for predictive data reduction. These methods anticipate and store data that is anticipated to be highly valuable for future analysis. 
Objectives and Scope of This Research:
The goal of this research is to effectively reduce data sizes while preserving the accuracy and integrity of the data. It does this by methodically investigating and validating the use of PCA in the domain of  Internet of Things. The main goals consist of:
- Effectiveness Evaluation: To determine how PCA can successfully reduce the dimensionality of Internet of Things data without appreciably losing information from a variety of sources.
- Impact Assessment: To make sure that important information is not lost during the reduction process, this study looks at how PCA affects data integrity and the precision of subsequent analysis.
- Performance Comparison: In typical Internet of Things applications, such predictive maintenance and real-time environmental monitoring, to compare the operational performance of PCA-reduced data against non-reduced data.
- Methodological Enhancements: Taking into account variables like real-time processing requirements and the heterogeneity of IoT data types, this study aims to explore possible adjustments and improvements to the conventional PCA approach that could optimize its applicability specifically for IoT data scenarios.

By addressing these goals, the research hopes to provide solid insights into the scalability and practical benefits of utilizing PCA in IoT scenarios, hence promoting more sustainable and efficient data management techniques in the age of big data.

Methodology:

Description of the PCA Technique and its Application to Reduce Data Dimensionality:
Principal Component Analysis (PCA) is a statistical method that reduces complexity in high-dimensional data while preserving trends and patterns. PCA works by determining the directions (principal components) that maximize the variance of the data. This is accomplished using either an eigenvalue decomposition of the data covariance matrix or a singular value decomposition of the data matrix, depending on the computing technique used. The original data points are then projected onto these primary components, producing a new dataset with fewer dimensions. In the case of IoT, where data streams are large and continuous, using PCA aids in reducing the dataset to its most important features, hence lowering the resources necessary for data storage and processing while preserving information.

Details on the Datasets To be Used, the Preprocessing Steps, and the Experimental Setup:
Potential datasets for the research, could be KDD, UNSW-NB15, ToN-IoT, CSE-CIC-IDS2018, CICIoT 2023, N-BaIoT Dataset, etc. The datasets that might be finalized for utilization in this research to assess the efficacy of PCA in IoT data reduction:
- ToN-IoT: This dataset is derived from the Telemetry datasets of Networked IoT devices, which include normal and malicious network traffic, as well as system logs from various IoT devices. 
- CICIoT 2023 Dataset: This dataset provides comprehensive network traffic statistics for cybersecurity studies in the IoT ecosystem. It was created by the Canadian Institute for Cybersecurity and includes complete recordings of network activities across a number of IoT devices during various attack vectors, such as DDoS, DoS, and other malicious incursions, as well as regular traffic for baseline comparisons.
- N-BaIoT Dataset: This dataset provides insights into the behavior of IoT devices infected with BASHLITE and Mirai botnets. It includes network traffic from nine commercial IoT devices. This dataset, created for the study of botnet characteristics in IoT contexts, contains a diverse variety of features collected from network traffic, giving significant data for creating and testing IoT-specific security solutions.


Implementation Environment and Tools:
The experimental setup for applying PCA to IoT datasets will make use of the Python programming environment, which is well-suited for data analysis and machine learning. The configuration will rely on many key libraries, including:
-NumPy: For efficient numerical computations. NumPy arrays provide a high-performance multidimensional array object that is essential for processing large data sets seen in IoT contexts. 
-Scikit-learn: A comprehensive Python machine learning toolkit with simple and efficient tools for data mining and analysis, including a well-supported PCA module. This library will be used to implement PCA by providing methods for fitting the model to the data and transforming the datasets into a reduced-dimensional space. 
 PCA Implementation Roadmap:
1.Data Preprocessing: Prior to applying PCA, the datasets will undergo many preprocessing steps:
 -Normalization and Scaling: The datasets' numerical features will be normalized to ensure that they contribute evenly to the analysis, preventing features with higher ranges from dominating the variance explained by the PCA.
  -Handling Missing Values: Any missing data points will be imputed or eliminated depending on their impact on overall dataset integrity and the volume of missing data.
  -Feature Encoding: Categorical features will be encoded using techniques like one-hot encoding to put them into a numerical format appropriate for PCA.

2.Applying PCA:
   -Fitting PCA: The Scikit-learn PCA object will be configured to keep components explaining at least 95% of the variance. This is an important stage since it balances dimensionality reduction and information retention. The 'PCA' class is created with the option 'n_components' set to a value that changes dynamically dependent on the components' cumulative explained variance ratio.
   - Data Transformation: After the PCA is applied to the data, the 'transform' method will be used to project the original data onto the space defined by the selected principle components. This will yield a new dataset with decreased dimensions, in which each data point has fewer features but preserves the majority of the original dataset's information.

3.Determination of Principal Components:
  -Explained Variance Ratio: After fitting the PCA, the explained variance ratio of each component will be examined. This statistic represents how much of the dataset's total variance is represented by each major component. Only components that together account for at least 95% of the variation are preserved.
  -Visual Inspection: A scree plot (a line plot of the eigenvalues of factors or principal components in an analysis) is usually used to identify the "elbow" point, which is the point at which the explained variance no longer increases dramatically with additional components. This graphic assistance will aid in confirming the quantity of components to preserve.

Evaluation of the Reduced Data:
After applying PCA, the reduced datasets will be examined to assess their structure and integrity.
-Dataset Structure: The structure of the changed datasets will be assessed to determine that the data reduction was carried out as planned, with a reduced number of features.
-Information Retention: Checks will be undertaken to ensure that the reduced datasets retain at least 95% of the original data's variance, preventing severe information loss during the reduction process.

The evaluation of PCA in the context of IoT data management is based on two primary criteria:
- Accuracy: This is determined by comparing the performance of machine learning models trained on original datasets to those learned on PCA-reduced datasets. Classification accuracy, precision, recall, and F1-score are tested to see if the reduced dimensionality affects the models' predictive ability.
- Efficiency: Efficiency is measured in terms of reduced data size and faster processing times. The reduction ratio, which measures the amount of data decreased, and the computational efficiency, which evaluates the time required to process the reduced data in comparison to the original data, are calculated.


![image](https://github.com/AnubhavGahlot/Principal-Component-Analysis-CSEN-493-49/assets/69597981/30c83b73-afeb-496a-967d-475e0c3a250b)
